\documentclass{report}
\input{./../../setup/preamble}
\graphicspath{{./../images/}}

\makeindex
% \phantom{\nabla_{u_i} \mathcal{L}:}&\quad \phantom{u_i} = -\lambda_{i+1} \\

\usepackage{subfiles} % Best loaded last in the preamble
\usepackage{titlesec}
\usepackage{booktabs}

\newcommand{\sectionbreak}{\clearpage}

\makeindex[columns=1]
\setcounter{tocdepth}{0}

\title{Math 610:Homework 2}
\author{Neal Kuperman}
\date{\today}

\lstset{
    backgroundcolor=\color{orange!10},
    frame=single,
    basicstyle=\ttfamily,
    breaklines=true,
    xleftmargin=0pt
}

\begin{document}

\maketitle


% ================================================================
% ================================================================
%                           ISLP 3.10 
% ================================================================
% ================================================================

{\noindent\Large\textbf{ISLP 3.10}\vspace{1em}} 

\noindent This question should be answered using the Carseats data set.

\begin{enumerate}[label=, leftmargin=1em]
    \item \hyperref[3.10.a]{(a)} Fit a multiple regression model to predict Sales using Price, Urban, and US.
    \item \hyperref[3.10.b]{(b)} Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!
    \item \hyperref[3.10.c]{(c)} Write out the model in equation form, being careful to handle the qualitative variables properly.
    \item \hyperref[3.10.d]{(d)} For which of the predictors can you reject the null hypothesis $H_0 : \beta_j =0$?
    \item \hyperref[3.10.e]{(e)} On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.
    \item \hyperref[3.10.f]{(f)} How well do the models in (a) and (e) fit the data?
    \item \hyperref[3.10.g]{(g)} Using the model from (e), obtain 95\% confidence intervals for
    the coefficient(s).
    \item \hyperref[3.10.h]{(h)} Is there evidence of outliers or high leverage observations in the
    model from (e)?
\end{enumerate}


{\vspace{1em}\noindent\textbf{Note(s)}\vspace{1em}}

The data is a simulated data set containing sales of child car seats at 400 different stores. Information on the data set can be found on the \href{https://islp.readthedocs.io/en/latest/datasets/Carseats.html}{ISLP documentation}.

\vspace{1em}

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
        \toprule
        \textbf{Variable} & \textbf{Description} \\
        \midrule
        Sales & Unit sales (in thousands) at each location \\
        CompPrice & Price charged by competitor at each location \\
        Income & Community income level (in thousands of dollars) \\
        Advertising & Local advertising budget for company at each location (in thousands of dollars) \\
        Population & Population size in region (in thousands) \\
        Price & Price company charges for car seats at each site \\
        ShelveLoc & Factor with levels Bad, Good, Medium — quality of shelving location \\
        Age & Average age of the local population \\
        Education & Education level at each location \\
        Urban & Factor (No/Yes) — whether store is in urban or rural location \\
        US & Factor (No/Yes) — whether store is in the US or not \\
        \bottomrule
    \end{tabular}
    \caption{Carseats Dataset Variables}
    \label{tab:carseats}
    \end{table}
\noindent\rule{\textwidth}{0.4pt}

{\vspace{1em}\noindent\Large\textbf{Solution}\vspace{1em}}


\label{3.10.a}\noindent\textbf{ISLP 3.10 (a)} 
\vspace{1em}

\noindent Table~\ref{tab:ols_carseats_1} shows the results of a linear model to predict Sales using Price, Urban, and US. The table was generated using the \texttt{statsmodels} library in Python.

\begin{table}[h]
\begin{center}
    \begin{tabular}{lclc}
    \toprule
    \textbf{Dep. Variable:}    &      Sales       & \textbf{  R-squared:         } &     0.239   \\
    \textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.234   \\
    \textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     41.52   \\
    \textbf{Date:}             & Tue, 13 Jan 2026 & \textbf{  Prob (F-statistic):} &  2.39e-23   \\
    \textbf{Time:}             &     18:41:11     & \textbf{  Log-Likelihood:    } &   -927.66   \\
    \textbf{No. Observations:} &         400      & \textbf{  AIC:               } &     1863.   \\
    \textbf{Df Residuals:}     &         396      & \textbf{  BIC:               } &     1879.   \\
    \textbf{Df Model:}         &           3      & \textbf{                     } &             \\
    \textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &             \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lcccccc}
                        & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
    \midrule
    \textbf{Intercept}  &      13.0435  &        0.651     &    20.036  &         0.000        &       11.764    &       14.323     \\
    \textbf{Price}      &      -0.0545  &        0.005     &   -10.389  &         0.000        &       -0.065    &       -0.044     \\
    \textbf{Urban\_Yes} &      -0.0219  &        0.272     &    -0.081  &         0.936        &       -0.556    &        0.512     \\
    \textbf{US\_Yes}    &       1.2006  &        0.259     &     4.635  &         0.000        &        0.691    &        1.710     \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lclc}
    \textbf{Omnibus:}       &  0.676 & \textbf{  Durbin-Watson:     } &    1.912  \\
    \textbf{Prob(Omnibus):} &  0.713 & \textbf{  Jarque-Bera (JB):  } &    0.758  \\
    \textbf{Skew:}          &  0.093 & \textbf{  Prob(JB):          } &    0.684  \\
    \textbf{Kurtosis:}      &  2.897 & \textbf{  Cond. No.          } &     628.  \\
    \bottomrule
    \end{tabular}
    %\caption{OLS Regression Results}
    \end{center}
    \caption{OLS Regression Results}
    \label{tab:ols_carseats_1}
    \vspace{1em}
\end{table}



\label{3.10.b}\noindent\textbf{ISLP 3.10 (b)} 

\begin{itemize}
    \item \textbf{Price}: Sales is negatively related to price, with sales decreasing by 54 units per dollar increase in price
    \item \textbf{Urban\_Yes}: There is not a significant statistical relationship between sales and whether or not a store is in an urban location
    \item \textbf{US\_Yes}: There is a positive relationship between price and whether a store is located in the US or not. On average, a store located in the US will sell 1200 more units than if they were located outside the US.
\end{itemize}

\label{3.10.c}\noindent\textbf{ISLP 3.10 (c)} 

    \[
    \text{Sales} = -0.0545 * \text{Price} + -0.0219 * \text{Urban\_Yes} + 1.2006 * \text{US\_Yes}
    \]

\label{3.10.d}\noindent\textbf{ISLP 3.10 (d)} 

The \textbf{Urban\_Yes} predictor has a P value of 0.936, meaning we can reject the null hypothesis for it. We can not reject the null hypothesis for any of the other variables.

\vspace{1em}

\label{3.10.e}\noindent\textbf{ISLP 3.10 (e)} 

Table~\ref{tab:ols_carseats_2} shows the results of a linear model to predict Sales using Price and US\_Yes. 

\begin{table}[H]
    \begin{center}
        \begin{tabular}{lclc}
        \toprule
        \textbf{Dep. Variable:}    &      Sales       & \textbf{  R-squared:         } &     0.239   \\
        \textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.235   \\
        \textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     62.43   \\
        \textbf{Date:}             & Tue, 13 Jan 2026 & \textbf{  Prob (F-statistic):} &  2.66e-24   \\
        \textbf{Time:}             &     20:11:19     & \textbf{  Log-Likelihood:    } &   -927.66   \\
        \textbf{No. Observations:} &         400      & \textbf{  AIC:               } &     1861.   \\
        \textbf{Df Residuals:}     &         397      & \textbf{  BIC:               } &     1873.   \\
        \textbf{Df Model:}         &           2      & \textbf{                     } &             \\
        \textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &             \\
        \bottomrule
        \end{tabular}
        \begin{tabular}{lcccccc}
                        & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
        \midrule
        \textbf{Intercept} &      13.0308  &        0.631     &    20.652  &         0.000        &       11.790    &       14.271     \\
        \textbf{Price}     &      -0.0545  &        0.005     &   -10.416  &         0.000        &       -0.065    &       -0.044     \\
        \textbf{US\_Yes}   &       1.1996  &        0.258     &     4.641  &         0.000        &        0.692    &        1.708     \\
        \bottomrule
        \end{tabular}
        \begin{tabular}{lclc}
        \textbf{Omnibus:}       &  0.666 & \textbf{  Durbin-Watson:     } &    1.912  \\
        \textbf{Prob(Omnibus):} &  0.717 & \textbf{  Jarque-Bera (JB):  } &    0.749  \\
        \textbf{Skew:}          &  0.092 & \textbf{  Prob(JB):          } &    0.688  \\
        \textbf{Kurtosis:}      &  2.895 & \textbf{  Cond. No.          } &     607.  \\
        \bottomrule
        \end{tabular}

        \caption{OLS Regression Results for Model with Price and US\_Yes}
        \label{tab:ols_carseats_2}
    \end{center}
\end{table}

\label{3.10.f}\noindent\textbf{ISLP 3.10 (f)} 

Models a and e fit the data equally poorly. Both have an $\text{R}^2$ value of 0.239

\vspace{1em}

\label{3.10.g}\noindent\textbf{ISLP 3.10 (g)} 

Table~\ref{tab:ols_carseats_3} shows the 95\% confidence intervals for the coefficients in the model from (e).

\begin{table}[h]
    \begin{center}
        \begin{tabular}{lrr}
    \toprule
    & 0 & 1 \\
    \midrule
    Intercept & 11.790320 & 14.271265 \\
    Price & -0.064760 & -0.044195 \\
    US\_Yes & 0.691520 & 1.707766 \\
    \bottomrule
\end{tabular}
\end{center}
\caption{95\% Confidence Intervals for Coefficients in Model from (e)}
\label{tab:ols_carseats_3}
\end{table}

\label{3.10.h}\noindent\textbf{ISLP 3.10 (h)} 

Table~\ref{tab:ols_carseats_4} shows the high leverage points in the model from (e). The Cook's Distances are plotted in Figure~\ref{fig:ols_carseats_2_cooks_d}.


\begin{table}[h]
    \begin{center}
        \begin{tabular}{rr}
            \toprule
            index & high\_leverage\_points \\
            \midrule
            42 & 0.043338 \\
            125 & 0.025966 \\
            155 & 0.016106 \\
            156 & 0.015356 \\
            159 & 0.015707 \\
            165 & 0.028567 \\
            171 & 0.021014 \\
            174 & 0.029687 \\
            191 & 0.018039 \\
            203 & 0.015356 \\
            208 & 0.018235 \\
            269 & 0.019195 \\
            272 & 0.018687 \\
            313 & 0.023165 \\
            315 & 0.017049 \\
            356 & 0.018279 \\
            365 & 0.017399 \\
            367 & 0.023707 \\
            383 & 0.016514 \\
            386 & 0.016555 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{High Leverage Points in Model from (e)}
    \label{tab:ols_carseats_4}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{3_10_h_cooks_d.png}
    \caption{Cook's Distances for Model from (e)}
    \label{fig:ols_carseats_2_cooks_d}
\end{figure}

\pagebreak


% ================================================================
% ================================================================
%                           ISLP 3.14 
% ================================================================
% ================================================================


{\noindent\Large\textbf{ISLP 3.14}\vspace{1em}} 

\noindent This problem focuses on the \textit{collinearity} problem.

\begin{enumerate}[label=, leftmargin=1em]
    \item \hyperref[3.14.a]{(a)} Perform the following commands in Python:
    \begin{lstlisting}[language=Python]
        rng = np.random.default_rng(10)
        x1 = rng.uniform(0, 1, size=100)
        x2 = 0.5 * x1 + rng.normal(size=100) / 10
        y = 2 + 2 * x1 + 0.3 * x2 + rng.normal(size=100)
    \end{lstlisting}
    The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model.
    What are the regression coefficients?

    \item \hyperref[3.14.b]{(b)} What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.
    \item \hyperref[3.14.c]{(c)} Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\beta}_2$? How do these relate to the true $\beta_0$, $\beta_1$ and $\beta_2$? Can you reject the null hypothesis $H_0 : \beta_1 = 0$? How about the null hypothesis $H_0 : \beta_2 = 0$?
    \item \hyperref[3.14.d]{(d)} Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$?
    \item \hyperref[3.14.e]{(e)} Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0 : \beta_2 = 0$?
    \item \hyperref[3.14.f]{(f)} Do the results obtained in (c)-(e) contradict each other? Explain your answer.
    \item \hyperref[3.14.g]{(g)} Suppose we obtain one additional observation, which was unfortunately mismeasured. We use the function np.concatenate() to add this additional observation to each of x1, x2 and y.
    \begin{lstlisting}[language=Python]
        x1 = np.concatenate([x1, [0.1]])
        x2 = np.concatenate([x2, [0.8]])
        y = np.concatenate([y, [6]])
    \end{lstlisting}
    Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.
\end{enumerate}

\noindent\rule{\textwidth}{0.4pt}

{\vspace{1em}\noindent\Large\textbf{Solution}\vspace{1em}}

\label{3.14.a}\noindent\textbf{ISLP 3.14 (a)} 

The linear model has the form
\begin{align*}
    y &= 2 + 2x_1 + 0.3x_2 + \epsilon \\
    \\
    \beta_0 &= 2 \\
    \beta_1 &= 2 \\
    \beta_2 &= 0.3 \\
\end{align*}

\vspace{1em}

\label{3.14.b}\noindent\textbf{ISLP 3.14 (b)} 
$$corr(x_1, x_2) = \frac{Cov(x_1x_2)}{\sigma_{x_1} * \sigma_{x_2}} = \frac{E[(x_1 - \overline{x_1})(x_2 - \overline{x_2})]}{\sigma_{x_1} * \sigma_{x_2}} = \frac{\sum_{i=1}^{n}(x_{1i} - \bar{x}_1)(x_{2i} - \bar{x}_2)}{\sqrt{\sum_{i=1}^{n}(x_{1i} - \bar{x}_1)^2 \cdot \sum_{i=1}^{n}(x_{2i} - \bar{x}_2)^2}}$$

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{3_14_b_scatterplot.png}
    \caption{Scatterplot of x1 and x2}
    \label{fig:3_14_b_scatterplot}
\end{figure}


\label{3.14.c}\noindent\textbf{ISLP 3.14 (c)} 

\begin{table}[H]
\begin{center}
    \begin{tabular}{lclc}
        \toprule
        \textbf{Dep. Variable:}    &        y         & \textbf{  R-squared:         } &     0.291   \\
        \textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.276   \\
        \textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     19.89   \\
        \textbf{Date:}             & Tue, 13 Jan 2026 & \textbf{  Prob (F-statistic):} &  5.76e-08   \\
        \textbf{Time:}             &     21:24:01     & \textbf{  Log-Likelihood:    } &   -130.62   \\
        \textbf{No. Observations:} &         100      & \textbf{  AIC:               } &     267.2   \\
        \textbf{Df Residuals:}     &          97      & \textbf{  BIC:               } &     275.1   \\
        \textbf{Df Model:}         &           2      & \textbf{                     } &             \\
        \textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &             \\
        \bottomrule
        \end{tabular}
        \begin{tabular}{lcccccc}
                    & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
        \midrule
        \textbf{const} &       1.9579  &        0.190     &    10.319  &         0.000        &        1.581    &        2.334     \\
        \textbf{x1}    &       1.6154  &        0.527     &     3.065  &         0.003        &        0.569    &        2.661     \\
        \textbf{x2}    &       0.9428  &        0.831     &     1.134  &         0.259        &       -0.707    &        2.592     \\
        \bottomrule
        \end{tabular}
        \begin{tabular}{lclc}
        \textbf{Omnibus:}       &  0.051 & \textbf{  Durbin-Watson:     } &    1.964  \\
        \textbf{Prob(Omnibus):} &  0.975 & \textbf{  Jarque-Bera (JB):  } &    0.041  \\
        \textbf{Skew:}          & -0.036 & \textbf{  Prob(JB):          } &    0.979  \\
        \textbf{Kurtosis:}      &  2.931 & \textbf{  Cond. No.          } &     11.9  \\
        \bottomrule
    \end{tabular}

    %\caption{OLS Regression Results}
    \end{center}
    \caption{OLS Regression Results for Model with x1 and x2}
    \label{tab:3_14_c_ols_results}
\end{table}

\vspace{1em}
\noindent The linear model obtained from the OLS is
$$ y = 1.9579 + 1.6154x_1 + 0.9428x_2$$
and 
\begin{table}[H]
    \begin{center}
        \begin{tabular}{lcc}
    \toprule
    \textbf{Variable} & \textbf{True} & \textbf{OLS Model} \\
    \midrule
    $\beta_0$ & 2.0 & 1.9579 \\
    $\beta_1$ & 2.0 & 1.6154 \\
    $\beta_2$ & 0.3 & 0.9428 \\
    \bottomrule
\end{tabular}
\end{center}
\caption{Coefficients for Model with x1 and x2}
\label{tab:3_14_c_coefficients}
\end{table}

We can reject the null hypothesis $H_0: \beta_1 = 0$ as its t value is large ($t_{\beta_1} = 3.065$) and P value is small ($P_{\beta_1} = 0.003$). However, we can not reject the null hypothesis $H_0: \beta_2 = 0$ because its t value is less than 2 ($t_{\beta_2} = 1.134$) along with a large P value ($P_{\beta_2} = 0.259$)

\vspace{1em}
\label{3.14.d}\noindent\textbf{ISLP 3.14 (d)} 

Table~\ref{tab:3_14_d_ols_results} shows the results of the OLS regression to predict y using only x1. The goodness of fit of the least squares regression did not change meaningfully when we elmiited x2 as a predictor variable ($R^2_{x1,x2} = 0.291$, $R^2_{x1} = 0.281$). We can reject the null hypothesis $H_0: \beta_1 = 0$ due to the large t value and small P value for x1


\begin{table}[H]
    \begin{center}
        \begin{tabular}{lcccccc}
                    & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
        \midrule
        \textbf{const} &       1.9371  &        0.189     &    10.242  &         0.000        &        1.562    &        2.312     \\
        \textbf{x1}    &       2.0771  &        0.335     &     6.196  &         0.000        &        1.412    &        2.742     \\
        \bottomrule
        \end{tabular}
        \begin{tabular}{lclc}
        \textbf{  R-squared:         } &     0.281 & \textbf{  Adj. R-squared:    } &     0.274  \\
        \textbf{  F-statistic:       } &     38.39 & \textbf{  Prob (F-statistic):} &  1.37e-08  \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{OLS Regression Results for Model with x1}
    \label{tab:3_14_d_ols_results}
\end{table}

\vspace{1em}
\label{3.14.e}\noindent\textbf{ISLP 3.14 (e)} 

Table~\ref{tab:3_14_e_ols_results} shows the results of the OLS regression to predict y using only x2. The goodness of fit of the least squares regression with only x2 included did not change meaningfully when we elmiited x1 as a predictor variable ($R^2_{x1,x2} = 0.291$, $R^2_{x2} = 0.222$). We can reject the null hypothesis $H_0: \beta_2 = 0$ due to the large t value and small P value for x2

\begin{table}[H]
    \begin{center}
        \begin{tabular}{lcccccc}
                    & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
        \midrule
        \textbf{const} &       2.3239  &        0.154     &    15.124  &         0.000        &        2.019    &        2.629     \\
        \textbf{x2}    &       2.9103  &        0.550     &     5.291  &         0.000        &        1.819    &        4.002     \\
        \bottomrule
        \end{tabular}
        \begin{tabular}{lclc}
        \textbf{  R-squared:         } &     0.222 & \textbf{  Adj. R-squared:    } &     0.214  \\
        \textbf{  F-statistic:       } &     27.99 & \textbf{  Prob (F-statistic):} &  7.43e-07  \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{OLS Regression Results for Model with x2}
    \label{tab:3_14_e_ols_results}
\end{table}

\vspace{1em}
\label{3.14.f}\noindent\textbf{ISLP 3.14 (f)} 

They do not contradict each other, x1 and x2 explain the variance in y equally well (poorly) with $R^2_{x1} = 0.281$ and $R^2_{x2} = 0.222$. Additionally the null hypothesis can not be rejected when only including 1 predictor, either x1 or x2. These outcomes can be explained by x1 and x2 being colinear (they are highly coorelated).

\vspace{1em}
\label{3.14.g}\noindent\textbf{ISLP 3.14 (g)} 

Table~\ref{tab:3_14_f_models} shows the results of the OLS regression to predict y using x1, x2, x1 only, and x2 only. Figure~\ref{fig:3_14_g} shows some diagnostic plots for the full model. 

\begin{table}[H]
    \begin{center}
    \textbf{Full Model} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 2.0618 & 10.7201 & 0.0000 & 0.1923 \\
        $x_1$ & 0.8575 & 1.8383 & 0.0690 & 0.4665 \\
        $x_2$ & 2.2663 & 3.2160 & 0.0018 & 0.7047 \\
        \midrule
        R-squared & 0.2916 & \multicolumn{2}{c}{F-statistic} & 20.1727 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{$x_1$ Only Model} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 2.0739 & 10.3098 & 0.0000 & 0.2012 \\
        $x_1$ & 1.8760 & 5.2360 & 0.0000 & 0.3583 \\
        \midrule
        R-squared & 0.2169 & \multicolumn{2}{c}{F-statistic} & 27.4159 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{$x_2$ Only Model} \\ \vspace{0.5em}
        \begin{tabular}{lcccc}
            \toprule
            & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
            \midrule
            intercept & 2.2840 & 15.0879 & 0.0000 & 0.1514 \\
            $x_2$ & 3.1458 & 6.0082 & 0.0000 & 0.5236 \\
            \midrule
            R-squared & 0.2672 & \multicolumn{2}{c}{F-statistic} & 36.0984 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{OLS Regression Results for Models with x1, x2, x1 only, and x2 only}
    \label{tab:3_14_f_models}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{3_14_g_vif.png}
    \caption{Diagnostic Plots for Full Model}
    \label{fig:3_14_g}
\end{figure}

\vspace{1em}
With the added point, we can no longer reject the null hypothesis for $\beta_1$, as the t value is now 1.8383 and the P value is now 0.0690. We can now, however, reject the null hypothesis for $\beta_2$, as the t value is now 3.2160 and the P value is now 0.0018. The added point is a high-leverage point and an outlier in the full model. All models fit the data equally well (poorly), with $R^2$ values all less than 0.3.

The added point does have a high leverage (0.347) but is not an outlier is an outlier because its Cook's Distance is not greater than 1 (Cook's Distance = 0.316).


\pagebreak


% ================================================================
% ================================================================
%                           ISLP 3.15 
% ================================================================
% ================================================================
{\noindent\Large\textbf{ISLP 3.15 (a, b, d)}\vspace{1em}} 

\noindent This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

\begin{enumerate}[label=, leftmargin=1em]
    \item \hyperref[3.15.a]{(a)} For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.
    \item \hyperref[3.15.b]{(b)} Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0 : \beta_j = 0$?
    \item \hyperref[3.15.d]{(d)} Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form
    \begin{align}
        y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon
    \end{align}
\end{enumerate}

\noindent\rule{\textwidth}{0.4pt}

{\vspace{1em}\noindent\Large\textbf{Solution}\vspace{1em}}

\label{3.15.a}\noindent\textbf{ISLP 3.15 (a)} 

\vspace{1em}
Figure~\ref{fig:3_15_a_scatter} shows the scatterplots for each predictor. Figure~\ref{fig:3_15_a_residuals} shows the residuals for each predictor. Table~\ref{tab:3_15_a_r_squared} shows the R-squared for each predictor. All R-squared values are less than 0.4, indicating that the univariatemodels are not very good at predicting the response.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{3_15_a_scatter.png}
    \caption{Scatterplots for Each Predictor}
    \label{fig:3_15_a_scatter}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{3_15_a_residuals.png}
    \caption{Residuals for Each Predictor}
    \label{fig:3_15_a_residuals}
\end{figure}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{lr}
        \toprule
        predictor & r\_squared \\
        \midrule
        rad & 0.391257 \\
        tax & 0.339614 \\
        lstat & 0.207591 \\
        nox & 0.177217 \\
        indus & 0.165310 \\
        medv & 0.150780 \\
        dis & 0.144149 \\
        age & 0.124421 \\
        ptratio & 0.084068 \\
        rm & 0.048069 \\
        zn & 0.040188 \\
        chas & 0.003124 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{R-squared for Each Predictor}
    \label{tab:3_15_a_r_squared}
\end{table}

\vspace{1em}
\label{3.15.b}\noindent\textbf{ISLP 3.15 (b)} 

Table~\ref{tab:3_15_b_ols_results} shows the results of the OLS regression to predict crim using all of the predictors. 
We can reject the null hypothesis, with a 95\% confidence interval $\alpha = 0.05$, for zn, dis, rad, and medv. We can not reject the null hypothesis for all other predictors. 

\vspace{1em}
\begin{table}[H]
    \begin{center}
        \begin{tabular}{lclc}
            \toprule
            \textbf{Dep. Variable:}    &       crim       & \textbf{  R-squared:         } &     0.449   \\
            \textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.436   \\
            \textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     33.52   \\
            \textbf{Date:}             & Tue, 13 Jan 2026 & \textbf{  Prob (F-statistic):} &  2.03e-56   \\
            \textbf{Time:}             &     23:53:03     & \textbf{  Log-Likelihood:    } &   -1655.4   \\
            \textbf{No. Observations:} &         506      & \textbf{  AIC:               } &     3337.   \\
            \textbf{Df Residuals:}     &         493      & \textbf{  BIC:               } &     3392.   \\
            \textbf{Df Model:}         &          12      & \textbf{                     } &             \\
            \textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &             \\
            \bottomrule
        \end{tabular}
        \begin{tabular}{lcccccc}
                                & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
            \midrule
            \textbf{Intercept} &      13.7784  &        7.082     &     1.946  &         0.052        &       -0.136    &       27.693     \\
            \textbf{zn}        &       0.0457  &        0.019     &     2.433  &         0.015        &        0.009    &        0.083     \\
            \textbf{indus}     &      -0.0584  &        0.084     &    -0.698  &         0.486        &       -0.223    &        0.106     \\
            \textbf{chas}      &      -0.8254  &        1.183     &    -0.697  &         0.486        &       -3.150    &        1.500     \\
            \textbf{nox}       &      -9.9576  &        5.290     &    -1.882  &         0.060        &      -20.351    &        0.436     \\
            \textbf{rm}        &       0.6289  &        0.607     &     1.036  &         0.301        &       -0.564    &        1.822     \\
            \textbf{age}       &      -0.0008  &        0.018     &    -0.047  &         0.962        &       -0.036    &        0.034     \\
            \textbf{dis}       &      -1.0122  &        0.282     &    -3.584  &         0.000        &       -1.567    &       -0.457     \\
            \textbf{rad}       &       0.6125  &        0.088     &     6.997  &         0.000        &        0.440    &        0.784     \\
            \textbf{tax}       &      -0.0038  &        0.005     &    -0.730  &         0.466        &       -0.014    &        0.006     \\
            \textbf{ptratio}   &      -0.3041  &        0.186     &    -1.632  &         0.103        &       -0.670    &        0.062     \\
            \textbf{lstat}     &       0.1388  &        0.076     &     1.833  &         0.067        &       -0.010    &        0.288     \\
            \textbf{medv}      &      -0.2201  &        0.060     &    -3.678  &         0.000        &       -0.338    &       -0.103     \\
            \bottomrule
        \end{tabular}
        \begin{tabular}{lclc}
            \textbf{Omnibus:}       & 663.436 & \textbf{  Durbin-Watson:     } &     1.516  \\
            \textbf{Prob(Omnibus):} &   0.000 & \textbf{  Jarque-Bera (JB):  } & 80856.852  \\
            \textbf{Skew:}          &   6.579 & \textbf{  Prob(JB):          } &      0.00  \\
            \textbf{Kurtosis:}      &  63.514 & \textbf{  Cond. No.          } &  1.24e+04  \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{OLS Regression Results for Multiple Regression Model}
    \label{tab:3_15_b_ols_results}
    Notes: \newline
     [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \newline
     [2] The condition number is large, 1.24e+04. This might indicate that there are \newline
     strong multicollinearity or other numerical problems.

\end{table}

\label{3.15.d}\noindent\textbf{ISLP 3.15 (d)} 

Tables~\ref{tab:3_15_d_poly_models_a} -~\ref{tab:3_15_d_poly_models_c} show the results of the polynomial regressions for each predictor. Figure~\ref{fig:3_15_d_scatter} and figure~\ref{fig:3_15_d_residuals} show the scatter and residuals plots for the polynomial regression models respectively.

Looking at the scatter plots, we can see that none of the variables have great non-linear fits to the response. Looking exclusively at the P values for the different polynomial models, we can not reject a non-linear relationship for indius, nox, age, dis, ptratio, and medv. 

\vspace{1em}
\begin{table}[H]
    \begin{center}
    \textbf{Model for zn:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 4.8461 & 11.1922 & 0.0000 & 0.4330 \\
        zn & -0.3322 & -3.0252 & 0.0026 & 0.1098 \\
        $\text{zn}^2$ & 0.0065 & 1.6791 & 0.0938 & 0.0039 \\
        $\text{zn}^3$ & -0.0000 & -1.2030 & 0.2295 & 0.0000 \\
        \midrule
        R-squared & 0.0582 & \multicolumn{2}{c}{F-statistic} & 10.3485 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{Model for indus:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 3.6626 & 2.3269 & 0.0204 & 1.5740 \\
        indus & -1.9652 & -4.0773 & 0.0001 & 0.4820 \\
        $\text{indus}^2$ & 0.2519 & 6.4070 & 0.0000 & 0.0393 \\
        $\text{indus}^3$ & -0.0070 & -7.2920 & 0.0000 & 0.0010 \\
        \midrule
        R-squared & 0.2597 & \multicolumn{2}{c}{F-statistic} & 58.6883 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{Model for chas:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 3.7444 & 9.4530 & 0.0000 & 0.3961 \\
        chas & -0.6309 & -1.2567 & 0.2094 & 0.5020 \\
        $\text{chas}^2$ & -0.6309 & -1.2567 & 0.2094 & 0.5020 \\
        $\text{chas}^3$ & -0.6309 & -1.2567 & 0.2094 & 0.5020 \\
        \midrule
        R-squared & 0.0031 & \multicolumn{2}{c}{F-statistic} & 1.5794 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{Model for nox:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 233.0866 & 6.9282 & 0.0000 & 33.6431 \\
        nox & -1279.3713 & -7.5082 & 0.0000 & 170.3975 \\
        $\text{nox}^2$ & 2248.5441 & 8.0334 & 0.0000 & 279.8993 \\
        $\text{nox}^3$ & -1245.7029 & -8.3446 & 0.0000 & 149.2816 \\
        \midrule
        R-squared & 0.2970 & \multicolumn{2}{c}{F-statistic} & 70.6867 \\
        \bottomrule
    \end{tabular}
    \end{center}
    \caption{Polynomial Regression Results for Each Predictor}
    \label{tab:3_15_d_poly_models_a}
\end{table}

\begin{table}[H]
    \begin{center}
        \vspace{1.5em}
        \textbf{Model for rm:} \\ \vspace{0.5em}
        \begin{tabular}{lcccc}
            \toprule
            & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
            \midrule
            intercept & 112.6246 & 1.7457 & 0.0815 & 64.5172 \\
            rm & -39.1501 & -1.2503 & 0.2118 & 31.3115 \\
            $\text{rm}^2$ & 4.5509 & 0.9084 & 0.3641 & 5.0099 \\
            $\text{rm}^3$ & -0.1745 & -0.6615 & 0.5086 & 0.2637 \\
            \midrule
            R-squared & 0.0678 & \multicolumn{2}{c}{F-statistic} & 12.1677 \\
            \bottomrule
        \end{tabular}
        
        \vspace{1.5em}
        \textbf{Model for age:} \\ \vspace{0.5em}
        \begin{tabular}{lcccc}
            \toprule
            & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
            \midrule
            intercept & -2.5488 & -0.9204 & 0.3578 & 2.7691 \\
            age & 0.2737 & 1.4683 & 0.1427 & 0.1864 \\
            $\text{age}^2$ & -0.0072 & -1.9878 & 0.0474 & 0.0036 \\
            $\text{age}^3$ & 0.0001 & 2.7237 & 0.0067 & 0.0000 \\
            \midrule
            R-squared & 0.1742 & \multicolumn{2}{c}{F-statistic} & 35.3061 \\
            \bottomrule
        \end{tabular}

        \vspace{1.5em}
        \textbf{Model for dis:} \\ \vspace{0.5em}
        \begin{tabular}{lcccc}
            \toprule
            & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
            \midrule
            intercept & 30.0476 & 12.2850 & 0.0000 & 2.4459 \\
            dis & -15.5544 & -8.9600 & 0.0000 & 1.7360 \\
            $\text{dis}^2$ & 2.4521 & 7.0783 & 0.0000 & 0.3464 \\
            $\text{dis}^3$ & -0.1186 & -5.8135 & 0.0000 & 0.0204 \\
            \midrule
            R-squared & 0.2778 & \multicolumn{2}{c}{F-statistic} & 64.3741 \\
            \bottomrule
        \end{tabular}

        \vspace{1.5em}
        \textbf{Model for rad:} \\ \vspace{0.5em}
        \begin{tabular}{lcccc}
            \toprule
            & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
            \midrule
            intercept & -0.6055 & -0.2954 & 0.7678 & 2.0501 \\
            rad & 0.5127 & 0.4913 & 0.6234 & 1.0436 \\
            $\text{rad}^2$ & -0.0752 & -0.5061 & 0.6130 & 0.1485 \\
            $\text{rad}^3$ & 0.0032 & 0.7031 & 0.4823 & 0.0046 \\
            \midrule
            R-squared & 0.4000 & \multicolumn{2}{c}{F-statistic} & 111.5727 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Polynomial Regression Results for Each Predictor}
    \label{tab:3_15_d_poly_models_b}
\end{table}


\begin{table}[H]
    \begin{center}
    \vspace{1.5em}
    \textbf{Model for tax:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 19.1836 & 1.6263 & 0.1045 & 11.7955 \\
        tax & -0.1533 & -1.6023 & 0.1097 & 0.0957 \\
        $\text{tax}^2$ & 0.0004 & 1.4877 & 0.1375 & 0.0002 \\
        $\text{tax}^3$ & -0.0000 & -1.1668 & 0.2439 & 0.0000 \\
        \midrule
        R-squared & 0.3689 & \multicolumn{2}{c}{F-statistic} & 97.8047 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{Model for ptratio:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 477.1840 & 3.0434 & 0.0025 & 156.7950 \\
        ptratio & -82.3605 & -2.9793 & 0.0030 & 27.6439 \\
        $\text{ptratio}^2$ & 4.6353 & 2.8821 & 0.0041 & 1.6083 \\
        $\text{ptratio}^3$ & -0.0848 & -2.7433 & 0.0063 & 0.0309 \\
        \midrule
        R-squared & 0.1138 & \multicolumn{2}{c}{F-statistic} & 21.4839 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{Model for lstat:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 1.2010 & 0.5920 & 0.5541 & 2.0286 \\
        lstat & -0.4491 & -0.9660 & 0.3345 & 0.4649 \\
        $\text{lstat}^2$ & 0.0558 & 1.8522 & 0.0646 & 0.0301 \\
        $\text{lstat}^3$ & -0.0009 & -1.5170 & 0.1299 & 0.0006 \\
        \midrule
        R-squared & 0.2179 & \multicolumn{2}{c}{F-statistic} & 46.6294 \\
        \bottomrule
    \end{tabular}

    \vspace{1.5em}
    \textbf{Model for medv:} \\ \vspace{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{coef} & \textbf{t value} & \textbf{p value} & \textbf{std err} \\
        \midrule
        intercept & 53.1655 & 15.8405 & 0.0000 & 3.3563 \\
        medv & -5.0948 & -11.7438 & 0.0000 & 0.4338 \\
        $\text{medv}^2$ & 0.1555 & 9.0455 & 0.0000 & 0.0172 \\
        $\text{medv}^3$ & -0.0015 & -7.3120 & 0.0000 & 0.0002 \\
        \midrule
        R-squared & 0.4202 & \multicolumn{2}{c}{F-statistic} & 121.2721 \\
        \bottomrule
    \end{tabular}
    \end{center}
    \caption{Polynomial Regression Results for Each Predictor}
    \label{tab:3_15_d_poly_models_c}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{3_15_d_scatter.png}
    \caption{Scatterplots for Polynomial Regression Models}
    \label{fig:3_15_d_scatter}
\end{figure}

\begin{figure}[H]
    \centering0
    \includegraphics[width=0.8\textwidth]{3_15_d_residuals.png}
    \caption{Residuals for Polynomial Regression Models}
    \label{fig:3_15_d_residuals}
\end{figure}

\pagebreak


{\noindent\Large\textbf{ESL 3.17}\vspace{1em}} 

\noindent Repeat the analysis of Table 3.3 on the spam data discussed in Chapter 1. Include LS, Best Subset, Ridge regression, and Lasso. (skip PCR and PLS)


{\vspace{2em}\noindent\textbf{Note(s)}\vspace{1em}}


\begin{itemize}
    \item \textbf{Best Subset}: Best-subset selection drops all variables with coefficients smaller than the Mth largest; this is a form of “hard-thresholding.”
    \item \textbf{Ridge regression}:
        $$
            \hat{\beta}^{\mathrm{ridge}} = 
            \underset{\beta}{\mathrm{argmin}} \left\{
                \sum_{i=1}^N 
                \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 
                + \lambda \sum_{j=1}^p \beta_j^2
            \right\}
            \qquad \text{with } \lambda \geq 1
        $$

        which can be written in matrix form as 
        $$
            \mathrm{RSS}(\lambda) = 
            \left(\mathbf{y} - \mathbf{X}\beta\right)^T 
                \left(\mathbf{y} - \mathbf{X}\beta\right) + 
                    \lambda\beta^2\beta
        $$
        with
        $$
            \hat{\beta}^{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}
        $$

    \item \textbf{Lasso}: Similar to ridge but with an $\mathrm{L}_1$ penelty instead of $\mathrm{L}_2$:
        $$
            \hat{\beta}^{\mathrm{lasso}} = 
            \underset{\beta}{\mathrm{argmin}} \left\{
                \sum_{i=1}^N 
                \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 
            \right\} 
        $$
        $$
            \text{subject to } \, \sum_{j=1}^p|\beta_j| \leq t
        $$

        or equivalently in *Lagrangian form*
        $$
            \hat{\beta}^{\mathrm{ridge}} = 
            \underset{\beta}{\mathrm{argmin}} \left\{
                \frac{1}{2}\sum_{i=1}^N 
                \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 
                + \lambda \sum_{j=1}^p |\beta_j|
            \right\}
            \qquad \text{with } \lambda \geq 1
        $$

        \vspace{1em}
        \textbf{NOTE}: t should be adaptively chosen to minimize an estimate of expected prediction error.

        \textbf{NOTE}: Ridge regression does a proportional shrinkage. Lasso translates each coefficient by a constant factor $\lambda$, truncating at zero (“soft thresholding)
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

{\vspace{1em}\noindent\Large\textbf{Solution}\vspace{1em}}

\pagebreak




\end{document}
 


