{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math 610: Homework 2\n",
        "**Neal Kuperman**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% Packages\n",
        "from ISLP import load_data, confusion_table\n",
        "from ISLP.models import ModelSpec as MS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import subplots\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as smf\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (r2_score, mean_squared_error, accuracy_score, confusion_matrix,\n",
        "                             roc_auc_score, classification_report, ConfusionMatrixDisplay, \n",
        "                             roc_curve, auc)\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "RANDOM_STATE = 1\n",
        "VERBOSE = False\n",
        "PRINT_LATEX = False\n",
        "SAVE_FIGS = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1: College Data Analysis\n",
        "\n",
        "Consider the `College` data from the ISLP package. Details about the data is described on page 65 of the ISLP textbook for this class ([https://islp.readthedocs.io/en/latest/datasets/College.html](https://islp.readthedocs.io/en/latest/datasets/College.html)).\n",
        "\n",
        "We would like to *predict* the **number of applications** received using the other variables.\n",
        "\n",
        "80% of the data (randomly generated) will be treated as training data. The rest will be the test data.\n",
        "\n",
        "---\n",
        "\n",
        "**a)** Fit a linear model using least squares and report the estimate of the test error.\n",
        "\n",
        "**b)** Fit a tree to the data. Summarize the results. Unless the number of terminal nodes is large, display the tree graphically. Report its MSE.\n",
        "\n",
        "**c)** Use Cross validation to determine whether pruning is helpful and determine the optimal size for the pruned tree. Compare the pruned and un-pruned trees. Report MSE for the pruned tree. Which predictors seem to be the most important?\n",
        "\n",
        "**d)** Use a bagging approach to analyze the data with B = 500 and B = 1000. Compute the MSE. Which predictors seem to be the most important?\n",
        "\n",
        "**e)** Repeat (d) with a random forest approach with B = 500 and B = 1000, and m â‰ˆ p = 3.\n",
        "\n",
        "**f)** Compare the results from the various methods. Which method would you recommend?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "college_original = load_data(\"College\")\n",
        "\n",
        "if VERBOSE:\n",
        "    display(college_original.head())\n",
        "    display(college_original.describe().round(3))\n",
        "\n",
        "if PRINT_LATEX:\n",
        "    desc = college_original.describe()\n",
        "\n",
        "    # Number of columns per table\n",
        "    cols_per_table = 6\n",
        "\n",
        "    # Get all column names\n",
        "    all_cols = desc.columns.tolist()\n",
        "\n",
        "    # Split columns into chunks of 5\n",
        "    for i in range(0, len(all_cols), cols_per_table):\n",
        "        cols_chunk = all_cols[i:i+cols_per_table]\n",
        "        desc_subset = desc[cols_chunk].style.format(\"{:.2f}\")\n",
        "        \n",
        "        print(f\"\\n# Table {i//cols_per_table + 1}: Columns {i+1}-{min(i+cols_per_table, len(all_cols))}\")\n",
        "        print(desc_subset.to_latex())\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before answering any of the questions, we need to process the data\n",
        "1) split into 80/20 train/test split\n",
        "2) Convert the Private variable to a numeric value using one-hot encoding or dummy in Pandas\n",
        "3) Scale numeric independent variables using the standard scalar transform from scikit-learn \n",
        "4) Add intercept column to train and test dfs which is needed when passing a dataframe into statsmodel OLS function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = college_original[\"Apps\"]\n",
        "X_original = college_original.drop(columns=[\"Apps\"])\n",
        "X_train_original, X_test_original, y_train, y_test = train_test_split(\n",
        "    X_original, y, test_size=0.3, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "#============================\n",
        "# One-hot encoding code\n",
        "#============================\n",
        "num_cols = X_original.select_dtypes(include=['number']).columns\n",
        "cat_cols = X_original.select_dtypes(include=['category']).columns\n",
        "transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "        (\"cat\", OneHotEncoder(sparse_output=False, drop='first'), cat_cols),\n",
        "    ],\n",
        "    remainder=\"passthrough\" \n",
        ")\n",
        "\n",
        "\n",
        "X_train_transformed = transformer.fit_transform(X_train_original)\n",
        "X_test_transformed = transformer.transform(X_test_original)\n",
        "feature_names = [name.split(\"__\", 1)[1] for name in transformer.get_feature_names_out()]\n",
        "X_train = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
        "X_test = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
        "X_train.insert(0, 'Intercept', 1.0)\n",
        "X_test.insert(0, 'Intercept', 1.0)\n",
        "\n",
        "if VERBOSE:\n",
        "    display(X_train.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**a)** Fit a Linear Model using least squares and report the estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear model and error report\n",
        "model = smf.OLS(y_train.values, X_train).fit()\n",
        "\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "mse_train = mean_squared_error(y_train.values, y_train_pred)\n",
        "mse_test = mean_squared_error(y_test.values, y_test_pred)\n",
        "r2_train = r2_score(y_train.values, y_train_pred)\n",
        "r2_test = r2_score(y_test.values, y_test_pred)\n",
        "\n",
        "if VERBOSE:\n",
        "    print(model.summary())\n",
        "\n",
        "    print(f\"Train MSE: {mse_train}\")\n",
        "    print(f\"Train R2: {r2_train}\")\n",
        "    print(f\"\\n\\nTest MSE: {mse_test}\")\n",
        "    print(f\"Test R2: {r2_test}\")\n",
        "\n",
        "\n",
        "    plt.plot(y_test.values, y_test_pred, 'o')\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.title('CollegeActual vs Predicted')\n",
        "    plt.show()\n",
        "\n",
        "if PRINT_LATEX:\n",
        "    print(model.summary().as_latex())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**b)** Fit a *regression* tree to the data. Summarize the results. Unless the number of terminal nodes is large, display the tree graphically. Report its MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_and_plot_tree(X_train, y_train, X_test, y_test, max_depth, save_fig=False):\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth, random_state=RANDOM_STATE, criterion=\"squared_error\")\n",
        "    tree.fit(X_train, y_train)\n",
        "    y_pred_tree = tree.predict(X_test)\n",
        "    y_pred_train = tree.predict(X_train)\n",
        "    train_mse_tree = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2_tree = r2_score(y_train, y_pred_train)\n",
        "    test_mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "    test_r2_tree = r2_score(y_test, y_pred_tree)\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(\"\\n\")\n",
        "        print(f\"Tree w/ max depth = {max_depth}\")\n",
        "        print(\"=\"*60)   \n",
        "\n",
        "        if max_depth < 5:\n",
        "            fig, ax = plt.subplots(figsize=(18, 10))\n",
        "            plot_tree(tree, max_depth=max_depth, feature_names=pd.get_dummies(X_train).columns, ax=ax, fontsize=10, filled=True)\n",
        "            ax.set_title(f\"Tree w/ max depth = {max_depth}\")\n",
        "            if save_fig and SAVE_FIGS:\n",
        "                plt.savefig(f\"../images/HW_2/prob_1_tree_max_depth_{max_depth}.png\", dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "        print(f\"training MSE: {train_mse_tree}\")\n",
        "        print(f\"training R2: {train_r2_tree}\")\n",
        "        print(f\"test MSE: {test_mse_tree}\")\n",
        "        print(f\"test R2: {test_r2_tree}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    return tree\n",
        "\n",
        "tree_depth_3 = fit_and_plot_tree(X_train, y_train, X_test, y_test, 3)\n",
        "tree_depth_10 = fit_and_plot_tree(X_train, y_train, X_test, y_test, 10)\n",
        "\n",
        "importances_3 = pd.Series(\n",
        "    tree_depth_3.feature_importances_,\n",
        "    index=pd.get_dummies(X_train).columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "importances_10 = pd.Series(\n",
        "    tree_depth_10.feature_importances_,\n",
        "    index=pd.get_dummies(X_train).columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "if VERBOSE:\n",
        "    print(importances_3.head(10))\n",
        "    print(importances_10.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**c)** Use Cross validation to determine whether pruning is helpful and determine the optimal size for the pruned tree. Compare the pruned and un-pruned trees. Report MSE for the pruned tree. Which predictors seem to be the most important?\n",
        "\n",
        "We will use the `cost_complexity_pruning_path()` method of the `DecisionTreeRegressor` class to extract cost-complexity values. We will use the tree with a max depth of 10 to see how much reduction in complexity the pruning can achieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = tree_depth_10.cost_complexity_pruning_path(X_train, y_train)\n",
        "alphas = path.ccp_alphas\n",
        "\n",
        "kfold = KFold(10,\n",
        "              random_state=RANDOM_STATE,\n",
        "              shuffle=True)\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeRegressor(random_state=RANDOM_STATE, criterion=\"squared_error\"),\n",
        "                        {'ccp_alpha': alphas},\n",
        "                        refit=True,\n",
        "                        cv=kfold,\n",
        "                        scoring='neg_mean_squared_error')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_alpha = grid.best_params_[\"ccp_alpha\"]\n",
        "best_cv_mse = -grid.best_score_\n",
        "\n",
        "if VERBOSE:\n",
        "    print(f\"Best alpha: {best_alpha}\")\n",
        "    print(f\"Best CV MSE: {best_cv_mse}\")\n",
        "    # print(f\"Best Score: {grid.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_tree = grid.best_estimator_\n",
        "\n",
        "y_train_pred = best_tree.predict(X_train)\n",
        "y_test_pred = best_tree.predict(X_test)\n",
        "\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)  \n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "if VERBOSE:\n",
        "    print(f\"Best Tree Training MSE: {train_mse}\")\n",
        "    print(f\"Best Tree Training R2: {train_r2}\")\n",
        "    print(f\"Best Tree Test MSE: {test_mse}\")\n",
        "    print(f\"Best Tree Test R2: {test_r2}\")\n",
        "    print(f\"Best Tree Leaves: {best_tree.get_n_leaves()}\")\n",
        "    print(f\"Best Tree Depth: {best_tree.get_depth()}\")\n",
        "    print(f\"Best Tree R2: {r2_score(y_test, y_test_pred)}\")\n",
        "\n",
        "    plt.plot(y_test.values, y_test_pred, 'o', label='best')\n",
        "    plt.plot(y_test.values, tree_depth_10.predict(X_test), 'o', label = \"10\")\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.title('Actual vs Predicted')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    ax = subplots(figsize=(12, 12))[1]\n",
        "    plot_tree(best_tree,\n",
        "            feature_names=feature_names,\n",
        "            ax=ax,\n",
        "            filled=True);\n",
        "    plt.title(f'Pruned Best Tree \\nalpha = {best_alpha:.3f}')\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig(f'../images/HW_2/pruned_best_tree_alpha_{best_alpha:.3f}.png', dpi=300)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances = pd.Series(\n",
        "    best_tree.feature_importances_,\n",
        "    index=pd.get_dummies(X_train).columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "if VERBOSE:\n",
        "    print(importances.head(10))\n",
        "if PRINT_LATEX:\n",
        "    print(importances.to_latex())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**d)** Use a bagging approach to analyze the data with B = 500 and B = 1000. Compute the MSE. Which predictors seem to be the most important?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bagging_regressor(X_train, y_train, X_test, y_test, n_estimators, save_fig=False):\n",
        "    \n",
        "    regressors = []\n",
        "    \n",
        "    if VERBOSE:\n",
        "        fig, axes = plt.subplots(1, len(n_estimators), figsize=(12, 5))\n",
        "\n",
        "    for i, n in enumerate(n_estimators):\n",
        "        bag_n = BaggingRegressor(\n",
        "            estimator=DecisionTreeRegressor(),\n",
        "            n_estimators=n,\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        regressors.append(bag_n)\n",
        "        bag_n.fit(X_train, y_train)\n",
        "\n",
        "        if VERBOSE:\n",
        "            y_train_pred_n = bag_n.predict(X_train)     \n",
        "            y_test_pred_n = bag_n.predict(X_test)\n",
        "            train_mse_bag_n = mean_squared_error(y_train, y_train_pred_n)\n",
        "            train_r2_bag_n = r2_score(y_train, y_train_pred_n)\n",
        "            test_mse_bag_n = mean_squared_error(y_test, y_test_pred_n)\n",
        "            test_r2_bag_n = r2_score(y_test, y_test_pred_n)\n",
        "            print(f\"Bagging {n} Training MSE: {train_mse_bag_n}\")\n",
        "            print(f\"Bagging {n} Training R2: {train_r2_bag_n}\")\n",
        "            print(f\"Bagging {n} Test MSE: {test_mse_bag_n}\")\n",
        "            print(f\"Bagging {n} Test R2: {test_r2_bag_n}\")\n",
        "\n",
        "            axes[i].scatter(y_test, y_test_pred_n, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
        "            axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "            axes[i].set_xlabel('Actual - Test Data')\n",
        "            axes[i].set_ylabel('Predicted - Test Data')\n",
        "            axes[i].set_title(f'Bagging (B={n})\\nR$^2$: {r2_score(y_test, y_test_pred_n):.2f}, MSE: {mean_squared_error(y_test, y_test_pred_n):.2f}') \n",
        "\n",
        "    if VERBOSE:\n",
        "        plt.tight_layout()\n",
        "\n",
        "    if save_fig and VERBOSE:\n",
        "        estimator_names = str.join(\"_\", [f\"B={n}\" for n in n_estimators])\n",
        "        if SAVE_FIGS:\n",
        "            plt.savefig(f'../images/HW_2/bagging_regressor_n_{estimator_names}.png', dpi=300)\n",
        "\n",
        "    if VERBOSE:\n",
        "        plt.show()\n",
        "\n",
        "    return regressors\n",
        "\n",
        "regressors = bagging_regressor(X_train, y_train, X_test, y_test, [500, 1000], save_fig=True)\n",
        "bag_500, bag_1000 = regressors[0], regressors[1]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_feature_importance(regressor):\n",
        "    imp = np.mean([\n",
        "        tree.feature_importances_\n",
        "        for tree in regressor.estimators_\n",
        "    ], axis=0)\n",
        "    display(pd.Series(imp, index=pd.get_dummies(X_train).columns)\\\n",
        "      .sort_values(ascending=False)\\\n",
        "      .head(10))\n",
        "\n",
        "if VERBOSE:\n",
        "    print(\"Feature Importance for Bagging (B=500)\")\n",
        "    print(\"=\"*60)\n",
        "    print_feature_importance(bag_500)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"Feature Importance for Bagging (B=1000)\")\n",
        "    print(\"=\"*60)\n",
        "    print_feature_importance(bag_1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**e)** Repeat (d) with a random forest approach with B = 500 and B = 1000, and m â‰ˆ p = 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "def RF_regressor(X_train, y_train, X_test, y_test, n_estimators, save_fig=False):\n",
        "    \n",
        "    regressors = []\n",
        "    \n",
        "    if VERBOSE:\n",
        "        fig, axes = plt.subplots(1, len(n_estimators), figsize=(12, 5))\n",
        "\n",
        "    for i, n in enumerate(n_estimators):\n",
        "        RF_n = RandomForestRegressor(\n",
        "            n_estimators=n,\n",
        "            max_features=3,\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        regressors.append(RF_n)\n",
        "\n",
        "        RF_n.fit(pd.get_dummies(X_train), y_train)\n",
        "\n",
        "        if VERBOSE:\n",
        "            y_train_pred_n = RF_n.predict(pd.get_dummies(X_train))     \n",
        "            y_test_pred_n = RF_n.predict(pd.get_dummies(X_test))\n",
        "            train_mse_RF_n = mean_squared_error(y_train, y_train_pred_n)\n",
        "            train_r2_RF_n = r2_score(y_train, y_train_pred_n)\n",
        "            test_mse_RF_n = mean_squared_error(y_test, y_test_pred_n)\n",
        "            test_r2_RF_n = r2_score(y_test, y_test_pred_n)\n",
        "            print(f\"RF w {n} estimators Training MSE: {train_mse_RF_n}\")\n",
        "            print(f\"RF w {n} estimators Training R2: {train_r2_RF_n}\")\n",
        "            print(f\"RF w {n} estimators Test MSE: {test_mse_RF_n}\")\n",
        "            print(f\"RF w {n} estimators Test R2: {test_r2_RF_n}\")\n",
        "\n",
        "            axes[i].scatter(y_test, y_test_pred_n, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
        "            axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "            axes[i].set_xlabel('Actual - Test Data')\n",
        "            axes[i].set_ylabel('Predicted - Test Data')\n",
        "            axes[i].set_title(f'RF (num estimators={n})\\nR$^2$: {r2_score(y_test, y_test_pred_n):.2f}, MSE: {mean_squared_error(y_test, y_test_pred_n):.2f}') \n",
        "\n",
        "    if VERBOSE:\n",
        "        plt.tight_layout()\n",
        "\n",
        "    if save_fig:\n",
        "        estimator_names = str.join(\"_\", [f\"estimators={n}\" for n in n_estimators])\n",
        "        if SAVE_FIGS:\n",
        "            plt.savefig(f'../images/HW_2/RF_regressor_n_{estimator_names}.png', dpi=300)\n",
        "\n",
        "    if VERBOSE:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return regressors\n",
        "    \n",
        "regressors = RF_regressor(X_train, y_train, X_test, y_test, [500, 1000], save_fig=False)\n",
        "RF_500, RF_1000 = regressors[0], regressors[1]\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_feature_importance(regressor):\n",
        "    imp = np.mean([\n",
        "        tree.feature_importances_\n",
        "        for tree in regressor.estimators_\n",
        "    ], axis=0)\n",
        "    display(pd.Series(imp, index=pd.get_dummies(X_train).columns)\\\n",
        "      .sort_values(ascending=False)\\\n",
        "      .head(10))\n",
        "\n",
        "if VERBOSE:\n",
        "    print(\"Feature Importance for Bagging (B=500)\")\n",
        "    print(\"=\"*60)\n",
        "    print_feature_importance(RF_500)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"Feature Importance for Bagging (B=1000)\")\n",
        "    print(\"=\"*60)\n",
        "    print_feature_importance(RF_1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**f)** Compare the results from the various methods. Which method would you recommend?\n",
        "\n",
        "I would recommend the random forest with bagging classifier. Although the linear regression model has a similar MSE for the test data, the random forest will work with non-linear relationships making it a more robust classifier. The major benefit to the linear regression model is the interpretability of the coefficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2: College Data Analysis\n",
        "\n",
        "Consider the business school admission data available in the admission.csv. The admission officer of a business school has used an *â€œindexâ€* of undergraduate grade point average (GPA,ð‘‹1) and graduate management aptitude test (GMAT,ð‘‹2) scores to help decide which applicants should be admitted to the schoolâ€™s graduate programs. This index is used to categorize each applicant into one of three groups â€“ admit (group 1), do not admit (group 2), and borderline (group 3). We will take the last ***four*** observations in **<u>each category</u>** as test data and the remaining observations as training data.\n",
        "\n",
        "\n",
        "\n",
        "**a)** Perform an exploratory analysis of the training data by examining appropriate plots and comment on how helpful these predictors may be in predicting response.\n",
        "\n",
        "**b)** Perform an LDA using the training data. Superimpose the decision boundary on an appropriate display of the data. Does the decision boundary seem sensible? In addition, compute the confusion matrix and overall misclassification rate based on both training and test data. What do you observe?\n",
        "\n",
        "**c)** Repeat (b) using QDA.\n",
        "\n",
        "**d)** Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\n",
        "\n",
        "**e)** Compare the results in (b), (c) and (d). Which classifier would you recommend? Justify your conclusions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</br>\n",
        "</br>\n",
        "\n",
        "**a)** Perform an exploratory analysis of the training data by examining appropriate plots and comment on how helpful these predictors may be in predicting response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "admin_data = pd.read_csv(\"admission.csv\")\n",
        "\n",
        "if VERBOSE:\n",
        "    display(HTML('<h3>Head of Admission Data</h3>'))\n",
        "    display(admin_data.head())\n",
        "    display(HTML('<h3>Description of Admission Data</h3>'))\n",
        "    display(admin_data.describe())\n",
        "    # print(admin_data.describe().to_latex())\n",
        "\n",
        "    # Check for missing values\n",
        "    display(HTML('<h3>Missing Values in Admission Data</h3>'))\n",
        "    display(admin_data.isnull().sum())\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data = admin_data[admin_data.groupby('Group').cumcount(ascending=False) >= 4]\n",
        "test_data = admin_data.groupby('Group').tail(4)\n",
        "# train_data = admin_data.iloc[:-4]\n",
        "# test_data = admin_data.iloc[-4:]\n",
        "y_train = train_data['Group']\n",
        "y_test = test_data['Group']\n",
        "X_train = train_data[['GPA', 'GMAT']]\n",
        "X_test = test_data[['GPA', 'GMAT']]\n",
        "\n",
        "# display(HTML(admin_data.to_html(max_rows=None)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "#EDA\n",
        "\n",
        "if VERBOSE:\n",
        "    # Check for missing values\n",
        "    missing_values = admin_data.isnull().sum()\n",
        "    print(\"Missing values in adminission data:\")\n",
        "    print(missing_values)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "    # Bar plot of Group counts\n",
        "    train_data['Group'].value_counts().plot(kind='bar', ax=axes[0], edgecolor='k')\n",
        "    axes[0].set_title('Group Distribution')\n",
        "    axes[0].set_xlabel('Group')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "    # Histogram for GPA\n",
        "    axes[1].hist(train_data['GPA'], bins=20, edgecolor='k', alpha=0.7)\n",
        "    axes[1].set_title('GPA Distribution')\n",
        "    axes[1].set_xlabel('GPA')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "\n",
        "    # Histogram for GMAT\n",
        "    axes[2].hist(train_data['GMAT'], bins=20, edgecolor='k', alpha=0.7)\n",
        "    axes[2].set_title('GMAT Distribution')\n",
        "    axes[2].set_xlabel('GMAT')\n",
        "    axes[2].set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if SAVE_FIGS:   \n",
        "        plt.savefig('../images/HW_2/admission_data_EDA_histograms.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    sns.scatterplot(data=admin_data, x='GPA', y='GMAT', hue='Group', palette='viridis')\n",
        "    plt.title('Scatter plot of GPA vs GMAT')\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig('../images/HW_2/admission_data_EDA_scatterplot.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    sns.boxplot(data=admin_data, x='Group', y='GPA', ax=axes[0])\n",
        "    axes[0].set_title('GPA Distribution by Group')\n",
        "\n",
        "    sns.boxplot(data=admin_data, x='Group', y='GMAT', ax=axes[1])\n",
        "    axes[1].set_title('GMAT Distribution by Group')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig('../images/HW_2/admission_data_EDA_boxplots.png', dpi=300)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**comment on how helpful these predictors may be in predicting response.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**b)** Perform an LDA using the training data. Superimpose the decision boundary on an appropriate display of the data. Does the decision boundary seem sensible? In addition, compute the confusion matrix and overall misclassification rate based on both training and test data. What do you observe?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Linear Discriminant Analysis\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "lda.fit(X_train, y_train)\n",
        "y_pred_test = lda.predict(X_test)\n",
        "y_pred_train = lda.predict(X_train)\n",
        "\n",
        "lda.classes_\n",
        "\n",
        "if VERBOSE:\n",
        "    display(HTML('<h3>Confusion Matrix for Training Data</h3>'))\n",
        "    display(confusion_table(y_pred_train, y_train.values, lda.classes_))\n",
        "\n",
        "    if PRINT_LATEX:\n",
        "        print(confusion_table(y_pred_train, y_train.values, lda.classes_).to_latex())\n",
        "\n",
        "    display(HTML('<h3>Confusion Matrix for Test Data</h3>'))\n",
        "    display(confusion_table(y_pred_test, y_test.values, lda.classes_))\n",
        "\n",
        "    if PRINT_LATEX:\n",
        "        print(confusion_table(y_pred_test, y_test.values, lda.classes_).to_latex())\n",
        "\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
        "    # Misclassification rate = (Total Samples - Correct Predictions) / Total Samples\n",
        "    misclass_rate_test = (cm_test.sum() - cm_test.diagonal().sum()) / cm_test.sum() \n",
        "    misclass_rate_train = (cm_train.sum() - cm_train.diagonal().sum()) / cm_train.sum() \n",
        "    misclass_rate_total = (cm_test.sum() - cm_test.diagonal().sum() + cm_train.sum() - cm_train.diagonal().sum()) / (cm_test.sum() + cm_train.sum())\n",
        "    print(f\"Misclassification rate for test data: {misclass_rate_test*100:.2f}%\")\n",
        "    print(f\"Misclassification rate for train data: {misclass_rate_train*100:.2f}%\")\n",
        "    print(f\"Misclassification rate for total data: {misclass_rate_total*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VERBOSE:\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "    # Plot the data points\n",
        "    scatter = ax.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], \n",
        "                        c=y_train, cmap='viridis', edgecolors='k', s=50)\n",
        "\n",
        "    # Get GPA range for plotting lines\n",
        "    gpa_range = np.linspace(X_train.iloc[:, 0].min() - 0.1, \n",
        "                            X_train.iloc[:, 0].max() + 0.1, 100)\n",
        "\n",
        "    # Plot decision boundary between each pair of classes\n",
        "    classes = lda.classes_\n",
        "    colors = ['red', 'blue', 'green']\n",
        "    pairs = [(0, 1), (0, 2), (1, 2)]  # class index pairs\n",
        "\n",
        "    for (i, j), color in zip(pairs, colors):\n",
        "        # Coefficients for the boundary line\n",
        "        coef_diff = lda.coef_[i] - lda.coef_[j]\n",
        "        intercept_diff = lda.intercept_[i] - lda.intercept_[j]\n",
        "        \n",
        "        # Solve for GMAT: coef_diff[0]*GPA + coef_diff[1]*GMAT + intercept_diff = 0\n",
        "        # GMAT = -(coef_diff[0]*GPA + intercept_diff) / coef_diff[1]\n",
        "        gmat_boundary = -(coef_diff[0] * gpa_range + intercept_diff) / coef_diff[1]\n",
        "        \n",
        "        ax.plot(gpa_range, gmat_boundary, color=color, linestyle='--', linewidth=2,\n",
        "                label=f'Boundary {classes[i]} vs {classes[j]}')\n",
        "\n",
        "    ax.set_xlim(X_train.iloc[:, 0].min() - 0.1, X_train.iloc[:, 0].max() + 0.1)\n",
        "    ax.set_ylim(X_train.iloc[:, 1].min() - 20, X_train.iloc[:, 1].max() + 20)\n",
        "    ax.set_xlabel('GPA')\n",
        "    ax.set_ylabel('GMAT')\n",
        "    ax.set_title('LDA Decision Boundaries (Analytical)')\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VERBOSE:\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "    # Plot decision boundaries (one line!)\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(\n",
        "        lda, \n",
        "        X_train, \n",
        "        response_method=\"predict\",\n",
        "        alpha=0.5,\n",
        "        ax=ax,\n",
        "        xlabel='GPA',\n",
        "        ylabel='GMAT'\n",
        "    )\n",
        "\n",
        "    # Overlay the actual data points\n",
        "    scatter_train = ax.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], \n",
        "                        c=y_train, edgecolor=\"k\", cmap='viridis')\n",
        "    scatter_test = ax.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], \n",
        "                        c=y_test, edgecolor=\"k\", cmap='viridis', marker='D', s=75)\n",
        "\n",
        "    # Create invisible scatter points just for legend\n",
        "    train_handle = ax.scatter([], [], marker='o', c='gray', edgecolor='k', label='Train')\n",
        "    test_handle = ax.scatter([], [], marker='D', c='gray', edgecolor='k', label='Test')\n",
        "\n",
        "    # Legend 1: Groups\n",
        "    legend1 = ax.legend(*scatter_train.legend_elements(), title=\"Group\", loc='upper left')\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend 2: Markers\n",
        "    ax.legend(handles=[train_handle, test_handle], title=\"Data\", loc='upper right')\n",
        "\n",
        "    ax.set_title('LDA Decision Boundaries')\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig('../images/HW_2/problem_2_lda_decision_boundaries.png', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Perform an QDA using the training data. Superimpose the decision boundary on an appropriate display of the data. Does the decision boundary seem sensible? In addition, compute the confusion matrix and overall misclassification rate based on both training and test data. What do you observe? -->\n",
        "\n",
        "**c)** Repeat (b) using QDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Linear Discriminant Analysis\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train)\n",
        "y_pred_test = qda.predict(X_test)\n",
        "y_pred_train = qda.predict(X_train)\n",
        "\n",
        "qda.classes_\n",
        "\n",
        "if VERBOSE:\n",
        "    display(HTML('<h3>Confusion Matrix for Training Data</h3>'))\n",
        "    display(confusion_table(y_pred_train, y_train.values, lda.classes_))\n",
        "    print(confusion_table(y_pred_train, y_train.values, lda.classes_).to_latex())\n",
        "\n",
        "    display(HTML('<h3>Confusion Matrix for Test Data</h3>'))\n",
        "    display(confusion_table(y_pred_test, y_test.values, lda.classes_))\n",
        "    print(confusion_table(y_pred_test, y_test.values, lda.classes_).to_latex())\n",
        "\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
        "    # Misclassification rate = (Total Samples - Correct Predictions) / Total Samples\n",
        "    misclass_rate_test = (cm_test.sum() - cm_test.diagonal().sum()) / cm_test.sum() \n",
        "    misclass_rate_train = (cm_train.sum() - cm_train.diagonal().sum()) / cm_train.sum() \n",
        "    misclass_rate_total = (cm_test.sum() - cm_test.diagonal().sum() + cm_train.sum() - cm_train.diagonal().sum()) / (cm_test.sum() + cm_train.sum())\n",
        "    print(f\"Misclassification rate for test data: {misclass_rate_test*100:.2f}%\")\n",
        "    print(f\"Misclassification rate for train data: {misclass_rate_train*100:.2f}%\")\n",
        "    print(f\"Misclassification rate for all data: {misclass_rate_total*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "if VERBOSE:\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "    # Plot decision boundaries (one line!)\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(\n",
        "        qda, \n",
        "        X_train, \n",
        "        response_method=\"predict\",\n",
        "        alpha=0.5,\n",
        "        ax=ax,\n",
        "        xlabel='GPA',\n",
        "        ylabel='GMAT'\n",
        "    )\n",
        "\n",
        "    # Overlay the actual data points\n",
        "    scatter_train = ax.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], \n",
        "                        c=y_train, edgecolor=\"k\", cmap='viridis')\n",
        "    scatter_test = ax.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], \n",
        "                        c=y_test, edgecolor=\"k\", cmap='viridis', marker='D', s=75)\n",
        "\n",
        "    # Create invisible scatter points just for legend\n",
        "    train_handle = ax.scatter([], [], marker='o', c='gray', edgecolor='k', label='Train')\n",
        "    test_handle = ax.scatter([], [], marker='D', c='gray', edgecolor='k', label='Test')\n",
        "\n",
        "    # Legend 1: Groups\n",
        "    legend1 = ax.legend(*scatter_train.legend_elements(), title=\"Group\", loc='upper left')\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend 2: Markers\n",
        "    ax.legend(handles=[train_handle, test_handle], title=\"Data\", loc='upper right')\n",
        "\n",
        "    ax.set_title('QDA Decision Boundaries')\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig('../images/HW_2/problem_2_qda_decision_boundaries.png', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**d)** Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "**Error rate**: percentage misclassification\n",
        "$$\\frac{\\text{Misclassified}}{\\text{Total}} = 1 - \\text{Accuracy}$$\n",
        "**Sensitivity** (*true positive rate*): measures the proportion of actual positives correctly identified as positive \n",
        "$$\\frac{\\text{Number of true positives}}{\\text{Number of true positives} + \\text{Number of false negatives}}$$\n",
        "**Specificity** (*true negative rate*): measures the proportion of actual negatives correctly identified. NOTE: Also called recall\n",
        "$$\\frac{\\text{Number of true negatives}}{\\text{Number of true negatives} + \\text{Number of false positives}}$$\n",
        "\n",
        "**Precision**: Ratio of the correctly predicted class to the total predicted class\n",
        "$$\\frac{\\text{Number of true negatives}}{\\text{Number of true positives} + \\text{Number of false positives}}$$\n",
        "**F1 Score**: A harmonic means between the Precision and Recall score\n",
        "$$2 \\times \\frac{\\text{Precision} \\times \\text{Specificity}}{\\text{Precision} + \\text{Specificity}}$$\n",
        "\n",
        "\n",
        "ROC Curve: A Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system by plotting the true-positive rate (sensitivity) against the false-positive rate (1-specificity) across various decision thresholds $^{[1]}$. For a KNN classifier, the predicted probability for a class is the proportion of the K neighbors belonging to that class (i.e., $P(\\text{class K}|x) = \\frac{\\text{number of neighbors in class}}{K}$). The ROC curve is generated by varying the probability threshold required to classify a point as positive. When building the ROC curve, we vary t, where $\\text{t} \\in [0, 1]$ with the decision rule being predict Class_n if $P(\\text{Class\\_n}|x) \\geq t$\n",
        "\n",
        "For a multiclass classification problem, we can no longer generate a single ROC curve since it is based ona binary classifier. We can use the one vs all scheme, which compares each class against all the others (assumed as one) [(scikit learn tutorial)](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)\n",
        "\n",
        "\n",
        "\n",
        "**NOTE**: Since KNN is a distance based method, we want to make sure that GMAT and GRE are on similar scales. We are going to use the standard scaling method from scikit learn on our training and test data.\n",
        "\n",
        "\n",
        "[1] https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a pipeline with scaler + KNN\n",
        "knn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Optimal KNN Model\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Test different K values to find optimal\n",
        "k_range = range(1, len(X_train_scaled)-1)\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = knn_pipeline.set_params(knn__n_neighbors=k)\n",
        "    # knn.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    train_errors.append(1 - accuracy_score(y_train, knn_pipeline.predict(X_train)))\n",
        "    test_errors.append(1 - accuracy_score(y_test, knn_pipeline.predict(X_test)))\n",
        "\n",
        "# Find optimal K (lowest test error)\n",
        "optimal_k = k_range[np.argmin(test_errors)]\n",
        "\n",
        "if VERBOSE:\n",
        "    print(f\"Optimal K: {optimal_k}\")\n",
        "\n",
        "    # Plot K vs Error Rate\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(k_range, train_errors, label='Train Error', marker='o')\n",
        "    plt.plot(k_range, test_errors, label='Test Error', marker='s')\n",
        "    plt.axvline(optimal_k, color='r', linestyle='--', label=f'Optimal K={optimal_k}')\n",
        "    plt.xlabel('K')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title('KNN: K vs Error Rate')\n",
        "    plt.legend()\n",
        "\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig('../images/HW_2/problem_2_knn_k_vs_error_rate.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Fit optimal KNN\n",
        "knn_opt = knn_pipeline.set_params(knn__n_neighbors=optimal_k)\n",
        "knn_opt.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = knn_opt.predict(X_train)\n",
        "y_test_pred = knn_opt.predict(X_test)\n",
        "\n",
        "if VERBOSE:\n",
        "    # Metrics on TRAINING data\n",
        "    print(\"\\n=== Training Data Metrics ===\")\n",
        "    print(f\"Training Error Rate: {1 - accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "    print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "\n",
        "    # Confusion matrix for training\n",
        "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "    print(f\"\\nConfusion Matrix (Train):\\n{cm_train}\")\n",
        "\n",
        "    # Per-class sensitivity (recall) and specificity\n",
        "    print(\"\\nPer-Class Metrics (Training):\")\n",
        "    print(classification_report(y_train, y_train_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate sensitivity & specificity for each class (one-vs-rest)\n",
        "classes = knn_opt.classes_\n",
        "for i, _cls in enumerate(classes):\n",
        "    # Binary: this class vs all others\n",
        "    y_binary = (y_train == _cls).astype(int)\n",
        "    pred_binary = (y_train_pred == _cls).astype(int)\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(y_binary, pred_binary).ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    \n",
        "    if VERBOSE:\n",
        "        print(f\"Class {_cls}: Sensitivity={sensitivity:.4f}, Specificity={specificity:.4f}\")\n",
        "\n",
        "# AUC (multiclass: one-vs-rest)\n",
        "if hasattr(knn_opt, 'predict_proba'):\n",
        "    y_train_proba = knn_opt.predict_proba(X_train)\n",
        "    auc_train = roc_auc_score(y_train, y_train_proba, multi_class='ovr')\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(f\"\\nAUC (Training, OvR): {auc_train:.4f}\")\n",
        "\n",
        "if VERBOSE:\n",
        "    # Metrics on TEST data\n",
        "    print(\"\\n=== Test Data Metrics ===\")\n",
        "    print(f\"Test Error Rate: {1 - accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "    print(f\"\\nConfusion Matrix (Test):\\n{cm_test}\")\n",
        "\n",
        "    # Confusion matrix display\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    ConfusionMatrixDisplay.from_estimator(knn_opt, X_train, y_train, ax=axes[0], cmap='Blues')\n",
        "    axes[0].set_title(f'KNN (K={optimal_k}) - Training')\n",
        "    ConfusionMatrixDisplay.from_estimator(knn_opt, X_test, y_test, ax=axes[1], cmap='Blues')\n",
        "    axes[1].set_title(f'KNN (K={optimal_k}) - Test')\n",
        "    plt.tight_layout()\n",
        "    if SAVE_FIGS:\n",
        "        plt.savefig('../images/HW_2/problem_2_knn_confusion_matrix.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "if VERBOSE:\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "    # Plot decision boundaries with HIGHER resolution for smoother appearance\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(\n",
        "        knn_opt, \n",
        "        X_train, \n",
        "        response_method=\"predict\",\n",
        "        alpha=0.5,\n",
        "        ax=ax,\n",
        "        xlabel='GPA',\n",
        "        ylabel='GMAT',\n",
        "        grid_resolution=200,  # Increased from default 100 for smoother boundaries\n",
        "        eps=0.1  # Slight padding\n",
        "    )\n",
        "\n",
        "    # Overlay the actual data points\n",
        "    scatter_train = ax.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], \n",
        "                        c=y_train, edgecolor=\"k\", cmap='viridis', s=50)\n",
        "    scatter_test = ax.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], \n",
        "                        c=y_test, edgecolor=\"k\", cmap='viridis', marker='D', s=75)\n",
        "\n",
        "    # Create invisible scatter points just for legend\n",
        "    train_handle = ax.scatter([], [], marker='o', c='gray', edgecolor='k', label='Train')\n",
        "    test_handle = ax.scatter([], [], marker='D', c='gray', edgecolor='k', label='Test')\n",
        "\n",
        "    # Legend 1: Groups\n",
        "    legend1 = ax.legend(*scatter_train.legend_elements(), title=\"Group\", loc='upper left')\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend 2: Markers\n",
        "    ax.legend(handles=[train_handle, test_handle], title=\"Data\", loc='upper right')\n",
        "\n",
        "    ax.set_title(f'KNN Decision Boundaries (K={optimal_k})')  # Fixed title\n",
        "    plt.savefig('../images/HW_2/problem_2_knn_decision_boundaries.png', dpi=300)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binarize labels for multiclass\n",
        "y_train_bin = label_binarize(y_train, classes=knn_opt.classes_)\n",
        "y_train_pred_prob = knn_opt.predict_proba(X_train)\n",
        "\n",
        "if VERBOSE:\n",
        "    # Plot ROC for each class\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, cls in enumerate(knn_opt.classes_):\n",
        "        fpr, tpr, _ = roc_curve(y_train_bin[:, i], y_train_pred_prob[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'Group {cls} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Chance level')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves: Multiclass (One-vs-Rest)')\n",
        "    plt.legend() \n",
        "    plt.savefig('../images/HW_2/problem_2_knn_roc_curve.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    auc_train = roc_auc_score(y_train, y_train_pred_prob, multi_class=\"ovr\", average=\"macro\")\n",
        "    print(\"KNN train AUC (macro OVR):\", auc_train)\n",
        "    # y_test_pred_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**e)** Compare the results in (b), (c) and (d). Which classifier would you recommend? Justify your conclusions.\n",
        "\n",
        " We can see that the KNN classifier outperforms the other two classifiers, with the KNN having a misclassification rate for the test data of 8.3\\% compared to 16.7\\% and 25\\% for QDA and LDA respectively. Given the lower test error rate and overall error rate, I would recommend the KNN classifier."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
